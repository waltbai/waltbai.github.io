<!DOCTYPE html> <html lang="zh"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 发表论文 | 白龙 </title> <meta name="author" content=" 白龙"> <meta name="description" content="完整论文列表请见本人&lt;b&gt;&lt;a href='https://scholar.google.com/citations?user=Zrd9pCMAAAAJ'&gt;谷歌学术主页&lt;/a&gt;&lt;/b&gt;。"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://waltbai.github.io/zh/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/zh/"> 白龙 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/zh/">个人简介 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/zh/publications/">发表论文 <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/zh/projects/">科研项目 </a> </li> <li class="nav-item "> <a class="nav-link" href="/zh/service/">学术服务 </a> </li> <li class="nav-item "> <a class="nav-link" href="/zh/cv/">简历 </a> </li> <li class="nav-item "> <a class="nav-link" href="/zh/contact/">联系方式 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/"> EN</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">发表论文</h1> <p class="post-description">完整论文列表请见本人<b><a href="https://scholar.google.com/citations?user=Zrd9pCMAAAAJ" rel="external nofollow noopener" target="_blank">谷歌学术主页</a></b>。</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL-2025</abbr> </div> <div id="acl-2025-3" class="col-sm-8"> <div class="title">Towards Robust Universal Information Extraction: Benchmark, Evaluation, and Solution</div> <div class="author"> Jizhao Zhu,  Akang Shi,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2503.03201" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>In this paper, we aim to enhance the robustness of Universal Information Extraction (UIE) by introducing a new benchmark dataset, a comprehensive evaluation, and a feasible solution. Existing robust benchmark datasets have two key limitations: 1) They generate only a limited range of perturbations for a single Information Extraction (IE) task, which fails to evaluate the robustness of UIE models effectively; 2) They rely on small models or handcrafted rules to generate perturbations, often resulting in unnatural adversarial examples. Considering the powerful generation capabilities of Large Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE, called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic perturbations across different IE tasks. Based on this dataset, we comprehensively evaluate existing UIE models and reveal that both LLM-based models and other models suffer from significant performance drops. To improve robustness and reduce training costs, we propose a data-augmentation solution that dynamically selects hard samples for iterative training based on the model’s inference loss. Experimental results show that training with only 15% of the data leads to an average 7.5% relative performance improvement across three IE tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL-2025</abbr> </div> <div id="acl-2025-2" class="col-sm-8"> <div class="title">G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models</div> <div class="author"> <span class="font-weight-bold">Long Bai</span>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a>,  <a href="https://www.chuatatseng.com/" rel="external nofollow noopener" target="_blank">Tat-Seng Chua</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL 2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2506.00445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/waltbai/G2S-TKG-forecasting" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts based on historical ones has received much attention. Recent studies have introduced Large Language Models (LLMs) for this task to enhance the models’ generalization abilities. However, these models perform forecasting via simultaneously learning two kinds of entangled knowledge in the TKG: (1) general patterns, i.e., invariant temporal structures shared across different scenarios; and (2) scenario information, i.e., factual knowledge engaged in specific scenario, such as entities and relations. As a result, the learning processes of these two kinds of knowledge may interfere with each other, which potentially impact the generalization abilities of the models. To enhance the generalization ability of LLMs on this task, in this paper, we propose a General-to-Specific learning framework (G2S) that disentangles the learning processes of the above two kinds of knowledge. In the general learning stage, we mask the scenario information in different TKGs and convert it into anonymous temporal structures. After training on these structures, the model is able to capture the general patterns across different TKGs. In the specific learning stage, we inject the scenario information into the structures via either in-context learning or fine-tuning modes. Experimental results show that G2S effectively improves the generalization abilities of LLMs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL-2025</abbr> </div> <div id="acl-2025-1" class="col-sm-8"> <div class="title">AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual Alignment</div> <div class="author"> Yuxin Zuo,  Wenxuan Jiang,  Wenxuan Liu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span>,  Hanbin Wang,  Yutao Zeng,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL 2025</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2411.04794" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual alignment. Our findings suggest that although LLMs also demonstrate promising cross-lingual alignment in Information Extraction, there remains significant imbalance across languages, revealing an underlying deficiency in the IE alignment. To address this issue, we propose AlignXIE, a powerful code-based LLM that significantly enhances cross-lingual IE alignment through two strategies. Firstly, AlignXIE formulates IE across different languages, especially non-English ones, as code generation tasks, standardizing the representation of various schemas using Python classes to ensure consistency of the same ontology in different languages and align the schema. Secondly, it incorporates an IE cross-lingual alignment phase through a translated instance prediction task proposed in this paper to align the extraction process, utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples, generated by our proposed LLM-based automatic pipeline for IE parallel data construction, with manual annotation to ensure quality. Ultimately, we obtain AlignXIE through multilingual IE instruction tuning. Although without training in 9 unseen languages, AlignXIE surpasses ChatGPT by 30.17% and SoTA by 20.03%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 63 IE benchmarks in Chinese and English under various settings, demonstrate that AlignXIE significantly enhances cross-lingual and multilingual IE through boosting the IE alignment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="arxiv-2025-1" class="col-sm-8"> <div class="title">Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction</div> <div class="author"> Wenxuan Liu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span>,  Yuxin Zuo,  Daozhu Xu,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>arXiv preprint</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2503.02628" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Developing a general-purpose extraction system that can extract events with massive types is a long-standing target in Event Extraction (EE). In doing so, the challenge comes from two aspects: 1) The absence of an efficient and effective annotation method. 2) The absence of a powerful extraction method can handle massive types. For the first challenge, we propose a collaborative annotation method based on Large Language Models (LLMs). Through collaboration among multiple LLMs, it first refines annotations of trigger words from distant supervision and then carries out argument annotation. Next, a voting phase consolidates the annotation preferences across different LLMs. Finally, we create the EEMT dataset, the largest EE dataset to date, featuring over 200,000 samples, 3,465 event types, and 6,297 role types. For the second challenge, we propose an LLM-based Partitioning EE method called LLM-PEE. To overcome the limited context length of LLMs, LLM-PEE first recalls candidate event types and then splits them into multiple partitions for LLMs to extract events. The results in the supervised setting show that LLM-PEE outperforms the state-of-the-art methods by 5.4 in event detection and 6.1 in argument extraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement compared to mainstream LLMs, demonstrating its strong generalization capabilities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://coling2025.org/" rel="external nofollow noopener" target="_blank">COLING-2025</a> </abbr> </div> <div id="coling-2025" class="col-sm-8"> <div class="title">Large Language Model-Based Event Relation Extraction with Rationales</div> <div class="author"> Zhilei Hu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of The 31st International Conference on Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2025.coling-main.500/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Event Relation Extraction (ERE) aims to extract various types of relations between different events within texts. Although Large Language Models (LLMs) have demonstrated impressive capabilities in many natural language processing tasks, existing ERE methods based on LLMs still face three key challenges: (1) Time Inefficiency: The existing pairwise method of combining events and determining their relations is time-consuming for LLMs. (2) Low Coverage: When dealing with numerous events in a document, the limited generation length of fine-tuned LLMs restricts the coverage of their extraction results. (3) Lack of Rationale: Essential rationales concerning the results that could enhance the reasoning ability of the model are overlooked. To address these challenges, we propose LLMERE, an LLM-based approach with rationales for the ERE task. LLMERE transforms ERE into a question-and-answer task that may have multiple answers. By extracting all events related to a specified event at once, LLMERE reduces time complexity from O(n2) to O(n), compared to the pairwise method. Subsequently, LLMERE enhances the coverage of extraction results by employing a partitioning strategy that highlights only a portion of the events in the document at a time. In addition to the extracted results, LLMERE is also required to generate corresponding rationales/reasons behind them, in terms of event coreference information or transitive chains of event relations. Experimental results on three widely used datasets show that LLMERE achieves significant improvements over baseline methods.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2024.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP-2024</a> </abbr> </div> <div id="emnlp-2024" class="col-sm-8"> <div class="title">A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning</div> <div class="author"> Zhongwu Chen,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Zhen Huang,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  Yong Dou </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.emnlp-main.81/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NLPCC-2024</abbr> </div> <div id="nlpcc-2024" class="col-sm-8"> <div class="title">Retrieval-Augmented Code Generation for Universal Information Extraction</div> <div class="author"> Yucan Guo,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  Yantao Liu,  Yutao Zeng,  Wenxuan Liu ,  Xiang Li,  Pan Yang,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 13th CCF International Conference on Natural Language Processing and Chinese Computing</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2311.02962" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2024.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL-2024</a> </abbr> </div> <div id="acl-2024" class="col-sm-8"> <div class="title">KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction</div> <div class="author"> <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Yutao Zeng,  Yuxin Zuo,  Weicheng Ren,  Wenxuan Liu,  Miao Su ,  Yucan Guo,  Yantao Liu ,  Xiang Li,  Zhilei Hu,  <span class="font-weight-bold">Long Bai</span> ,  Wei Li,  Yidan Liu,  Pan Yang,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.acl-long.475/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/ICT-GoKnow/KnowCoder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over 30,000 types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around 1.5B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by 49.8% F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to 12.5% and 21.9%, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to 7.5% under the supervised setting.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="arxiv-2024-1" class="col-sm-8"> <div class="title">Temporal Knowledge Graph Question Answering: A Survey</div> <div class="author"> Miao Su,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Zhuo Chen,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a> </div> <div class="periodical"> <em>arXiv preprint</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2406.14191" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Knowledge Base Question Answering (KBQA) has been a long-standing field to answer questions based on knowledge bases. Recently, the evolving dynamics of knowledge have attracted a growing interest in Temporal Knowledge Graph Question Answering (TKGQA), an emerging task to answer temporal questions. However, this field grapples with ambiguities in defining temporal questions and lacks a systematic categorization of existing methods for TKGQA. In response, this paper provides a thorough survey from two perspectives: the taxonomy of temporal questions and the methodological categorization for TKGQA. Specifically, we first establish a detailed taxonomy of temporal questions engaged in prior studies. Subsequently, we provide a comprehensive review of TKGQA techniques of two categories: semantic parsing-based and TKG embedding-based. Building on this review, the paper outlines potential research directions aimed at advancing the field of TKGQA. This work aims to serve as a comprehensive reference for TKGQA and to stimulate further research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://ksem24.ai-edge.net/" rel="external nofollow noopener" target="_blank">KSEM-2024</a> </abbr> </div> <div id="ksem-2024" class="col-sm-8"> <div class="title">An In-Context Schema Understanding Method for Knowledge Base Question Answering</div> <div class="author"> Yantao Liu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a> ,  Yucan Guo,  <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 17th International Conference on Knowledge Science, Engineering and Management</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-981-97-5492-2_32" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The Knowledge Base Question Answering (KBQA) task aims to answer natural language questions based on a given knowledge base. Recently, Large Language Models (LLMs) have shown strong capabilities in language understanding and can be used to solve this task. In doing so, a major challenge for LLMs is to overcome the immensity and heterogeneity of knowledge base schemas.Existing methods bypass this challenge by initially employing LLMs to generate drafts of logic forms without schema-specific details. Then, an extra module is used to inject schema information to these drafts. In contrast, in this paper, we propose a simple In-Context Schema Understanding (ICSU) method that enables LLMs to directly understand schemas by leveraging in-context learning. Specifically, ICSU provides schema information to LLMs using schema-related annotated examples. We investigate three example retrieval strategies based on raw questions, anonymized questions, and generated SPARQL queries. Experimental results show that ICSU demonstrates competitive performance compared to baseline methods on both the KQA Pro and WebQSP datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://lrec-coling-2024.org/" rel="external nofollow noopener" target="_blank">COLING-2024</a> </abbr> </div> <div id="coling-2024-3" class="col-sm-8"> <div class="title">Class-Incremental Few-Shot Event Detection</div> <div class="author"> Kailin Zhao,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.lrec-main.290/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teacher model derives the current student model via adaptation. On the other hand, in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism. Extensive experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate the superior performance of Prompt-KD.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://lrec-coling-2024.org/" rel="external nofollow noopener" target="_blank">COLING-2024</a> </abbr> </div> <div id="coling-2024-2" class="col-sm-8"> <div class="title">Nested Event Extraction upon Pivot Element Recogniton</div> <div class="author"> Weicheng Ren,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  Miao Su,  Yantao Liu,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.lrec-main.1061/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/waysonren/PerNee" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer-nest events and as triggers of inner-nest events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner-nest and outer-nest events and further recognizes the PEs via classifying the relation type between trigger pairs. The model uses prompt learning to incorporate information from both event types and argument roles for better trigger and argument representations to improve NEE performance. Since existing NEE datasets (e.g., Genia11) are limited to specific domains and contain a narrow range of event types with nested structures, we systematically categorize nested events in the generic domain and construct a new NEE dataset, called ACE2005-Nest. Experimental results demonstrate that PerNee consistently achieves state-of-the-art performance on ACE2005-Nest, Genia11, and Genia13. The ACE2005-Nest dataset and the code of the PerNee model are available at https://github.com/waysonren/PerNee .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://lrec-coling-2024.org/" rel="external nofollow noopener" target="_blank">COLING-2024</a> </abbr> </div> <div id="coling-2024-1" class="col-sm-8"> <div class="title">Selective Temporal Knowledge Graph Reasoning</div> <div class="author"> Zhongni Hou,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2024.lrec-main.1268/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts in the form of (subject, relation, object, timestamp), has attracted much attention recently. TKG reasoning aims to predict future facts based on given historical ones. However, existing TKG reasoning models are unable to abstain from predictions they are uncertain, which will inevitably bring risks in real-world applications. Thus, in this paper, we propose an abstention mechanism for TKG reasoning, which helps the existing models make selective, instead of indiscriminate, predictions. Specifically, we develop a confidence estimator, called Confidence Estimator with History (CEHis), to enable the existing TKG reasoning models to first estimate their confidence in making predictions, and then abstain from those with low confidence. To do so, CEHis takes two kinds of information into consideration, namely, the certainty of the current prediction and the accuracy of historical predictions. Experiments with representative TKG reasoning models on two benchmark datasets demonstrate the effectiveness of the proposed CEHis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JCIP-2024</abbr> </div> <div id="jcip-2024" class="col-sm-8"> <div class="title">基于多历史序列联合演化建模的两阶段时序知识图谱推理</div> <div class="author"> <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank"> 李紫宣</a>,  官赛萍,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank"> 靳小龙</a>,  <span class="font-weight-bold"> 白龙</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank"> 郭嘉丰</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank"> 程学旗</a> </div> <div class="periodical"> <em>中文信息学报</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://jcip.cipsc.org.cn/CN/Y2024/V38/I2/46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>近年来，随着互联网技术和应用模式的迅猛发展，互联网数据规模爆炸式增长，其中包含大量带有时序信息的动态事件知识。为了建模这类动态事件知识，时序知识图谱在传统知识图谱的基础上引入时间信息，以带时间戳的知识图谱序列刻画这类知识。时序知识图谱推理任务旨在根据过去发生的事件四元组（主语实体，关系（事件类型），宾语实体，时间戳）预测未来发生的事件。为此，模型需要充分建模实体的历史演化过程。然而，巨大的实体数目以及它们对应的大量历史事件给时序知识图谱推理任务带来了巨大挑战。为了降低待建模历史的规模，已有方法选择建模查询实体的长程历史或者全部实体的短程历史，都丢失了一部分历史信息。实际上，由于不同实体对于一个查询的相关程度不同，模型需要更充分地建模相关实体的历史信息。基于此，该文提出了基于多历史序列联合演化建模的两阶段时序推理模型MENet（Multi-sequence Evolution Network）。具体而言，其在第一阶段采用了一种基于启发式规则的候选实体筛选策略，选择最有可能发生事件的候选实体，从而有效地降低了需要建模的实体数目；在第二阶段，其采用了一个多历史序列联合演化模型: 首先通过组合多个实体各自的长程历史信息，得到需要建模的图序列，进而通过考虑该图序列上同时刻发生事件之间的结构依赖、事件发生的时间数值信息以及不同时刻之间的时序依赖，从而更精准地建模实体演化过程。在三个标准数据集上的实验结果表明，上述模型相比于当前最先进的方法模型具有更好的推理性能。</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://aaai.org/aaai-conference/" rel="external nofollow noopener" target="_blank">AAAI-2024</a> </abbr> </div> <div id="aaai-2024" class="col-sm-8"> <div class="title">Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models</div> <div class="author"> Kun Zhang,  Jiali Zeng,  Fandong Meng,  Yuanzhuo Wang,  Shiqi Sun,  <span class="font-weight-bold">Long Bai</span>,  Huawei Shen,  Jie Zhou </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29928" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have recently demonstrated remarkable performance across various Natual Language Processing tasks. In the field of multi-hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a paradigm, using curated stepwise reasoning demonstrations to enhance LLM’s ability to reason and produce coherent rational pathways. To ensure the accuracy, reliability, and traceability of the generated answers, many studies have incorporated information retrieval (IR) to provide LLMs with external knowledge. However, existing CoT with IR methods decomposes questions into sub-questions based on a single compositionality type, which limits their effectiveness for questions involving multiple compositionality types. Additionally, these methods suffer from inefficient retrieval, as complex questions often contain abundant information, leading to the retrieval of irrelevant information inconsistent with the query’s intent. In this work, we propose a novel question decomposition framework called TRQA for multi-hop question answering, which addresses these limitations. Our framework introduces a reasoning tree (RT) to represent the structure of complex questions. It consists of four components: the Reasoning Tree Constructor (RTC), the Question Generator (QG), the Retrieval and LLM Interaction Module (RAIL), and the Answer Aggregation Module (AAM). Specifically, the RTC predicts diverse sub-question structures to construct the reasoning tree, allowing a more comprehensive representation of complex questions. The QG generates sub-questions for leaf-node in the reasoning tree, and we explore two methods for QG: prompt-based and T5-based approaches. The IR module retrieves documents aligned with sub-questions, while the LLM formulates answers based on the retrieved information. Finally, the AAM aggregates answers along the reason tree, producing a definitive response from bottom to top.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2023.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP-2023</a> </abbr> </div> <div id="emnlp-2023" class="col-sm-8"> <div class="title">Temporal Knowledge Graph Reasoning Based on N-tuple Modeling</div> <div class="author"> Zhongni Hou,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  Yutao Zeng,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2023</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2023.findings-emnlp.77" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/mazi-hou/N-TKGs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Reasoning over Temporal Knowledge Graphs (TKGs) that predicts temporal facts (e.g., events) in the future is crucial for many applications. The temporal facts in existing TKGs only contain their core entities (i.e., the entities playing core roles therein) and formulate them as quadruples, i.e., (subject entity, predicate, object entity, timestamp). This formulation oversimplifies temporal facts and inevitably causes information loss. Therefore, we propose to describe a temporal fact more accurately as an n-tuple, containing not only its predicate and core entities, but also its auxiliary entities, as well as the roles of all entities. By so doing, TKGs are augmented to N-tuple Temporal Knowledge Graphs (N-TKGs). To conduct reasoning over N-TKGs, we further propose N-tuple Evolutional Network (NE-Net). It recurrently learns the evolutional representations of entities and predicates in temporal facts at different timestamps in the history via modeling the relations among those entities and predicates. Based on the learned representations, reasoning tasks at future timestamps can be realized via task-specific decoders. Experiment results on two newly built datasets demonstrate the superiority of N-TKG and the effectiveness of NE-Net.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> </div> <div id="arxiv-2023-1" class="col-sm-8"> <div class="title">ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction</div> <div class="author"> Zhilei Hu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Daozhu Xu,  <span class="font-weight-bold">Long Bai</span> ,  Cheng Jin,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>arXiv preprint</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2309.12892" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Event Relation Extraction (ERE) aims to extract multiple kinds of relations among events in texts. However, existing methods singly categorize event relations as different classes, which are inadequately capturing the intrinsic semantics of these relations. To comprehensively understand their intrinsic semantics, in this paper, we obtain prototype representations for each type of event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework for the joint extraction of multiple kinds of event relations. Specifically, ProtoEM extracts event relations in a two-step manner, i.e., prototype representing and prototype matching. In the first step, to capture the connotations of different event relations, ProtoEM utilizes examples to represent the prototypes corresponding to these relations. Subsequently, to capture the interdependence among event relations, it constructs a dependency graph for the prototypes corresponding to these relations and utilized a Graph Neural Network (GNN)-based module for modeling. In the second step, it obtains the representations of new event pairs and calculates their similarity with those prototypes obtained in the first step to evaluate which types of event relations they belong to. Experimental results on the MAVEN-ERE dataset demonstrate that the proposed ProtoEM framework can effectively represent the prototypes of event relations and further obtain a significant improvement over baseline models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://kdd.org/kdd2023/" rel="external nofollow noopener" target="_blank">KDD-2023</a> </abbr> </div> <div id="kdd-2023" class="col-sm-8"> <div class="title">CFGL-LCR: A Counterfactual Graph Learning Framework for Legal Case Retrieval</div> <div class="author"> Kun Zhang,  Chong Chen,  Yuanzhuo Wang,  Qi Tian,  <span class="font-weight-bold">Long Bai</span> </div> <div class="periodical"> <em>In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599273" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Legal case retrieval, which aims to find relevant cases based on a short case description, serves as an important part of modern legal systems. Despite the success of existing retrieval methods based on Pretrained Language Models, there are still two issues in legal case retrieval that have not been well considered before. First, existing methods underestimate the semantics associations among legal elements, e.g., law articles and crimes, which played an essential role in legal case retrieval. These methods only adopt the pre-training language model to encode the whole legal case, instead of distinguishing different legal elements in the legal case. They randomly split a legal case into different segments, which may break the completeness of each legal element. Second, due to the difficulty in annotating the relevant labels of similar cases, legal case retrieval inevitably faces the problem of lacking training data. In this paper, we propose a counterfactual graph learning framework for legal case retrieval. Concretely, to overcome the above challenges, we transform the legal case document into a graph and model the semantics of the legal elements through a graph neural network. To alleviate the low resource and learn the causal relationship between the semantics of legal elements and relevance, a counterfactual data generator is designed to augment counterfactual data and enhance legal case representation. Extensive experiments based on two publicly available legal benchmarks demonstrate that our CFGL-LCR can significantly outperform previous state-of-the-art methods in legal case retrieval.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2023.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL-2023</a> </abbr> </div> <div id="acl-2023" class="col-sm-8"> <div class="title">Semantic Structure Enhanced Event Causality Identification</div> <div class="author"> Zhilei Hu,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2023.acl-long.610" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn).It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://aaai-23.aaai.org/" rel="external nofollow noopener" target="_blank">AAAI-2023</a> </abbr> </div> <div id="aaai-2023" class="col-sm-8"> <div class="title">Rich Event Modeling for Script Event Prediction</div> <div class="author"> <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26478" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/waltbai/RePredictor" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Script is a kind of structured knowledge extracted from texts, which contains a sequence of events. Based on such knowledge, script event prediction aims to predict the subsequent event. To do so, two aspects should be considered for events, namely, event description (i.e., what the events should contain) and event encoding (i.e., how they should be encoded). Most existing methods describe an event by a verb together with a few core arguments (i.e., subject, object, and indirect object), which are not precise enough. In addition, existing event encoders are limited to a fixed number of arguments, which are not flexible enough to deal with extra information. Thus, in this paper, we propose the Rich Event Prediction (REP) framework for script event prediction. Fundamentally, it is based on the proposed rich event description, which enriches the existing ones with three kinds of important information, namely, the senses of verbs, extra semantic roles, and types of participants. REP contains an event extractor to extract such information from texts. Based on the extracted rich information, a predictor then selects the most probable subsequent event. The core component of the predictor is a transformer-based event encoder that integrates the above information flexibly. Experimental results on the widely used Gigaword Corpus show the effectiveness of the proposed framework.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP-2022</a> </abbr> </div> <div id="emnlp-2022-2" class="col-sm-8"> <div class="title">Knowledge-Enhanced Self-Supervised Prototypical Network for Few-Shot Event Detection</div> <div class="author"> Kailin Zhao,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2022</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.findings-emnlp.467" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Prototypical network based joint methods have attracted much attention in few-shot event detection, which carry out event detection in a unified sequence tagging framework. However, these methods suffer from the inaccurate prototype representation problem, due to two main reasons: the number of instances for calculating prototypes is limited; And, they do not well capture the relationships among event prototypes. To deal with this problem, we propose a Knowledge-Enhanced self-supervised Prototypical Network, called KE-PN, for few-shot event detection. KE-PN adopts hybrid rules, which can automatically align event types to an external knowledge base, i.e., FrameNet, to obtain more instances. It proposes a self-supervised learning method to filter out noisy data from enhanced instances. KE-PN is further equipped with an auxiliary event type relationship classification module, which injects the relationship information into representations of event prototypes. Extensive experiments on three benchmark datasets, i.e., FewEvent, MAVEN, and ACE2005 demonstrate the state-of-the-art performance of KE-PN.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2022.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP-2022</a> </abbr> </div> <div id="emnlp-2022-1" class="col-sm-8"> <div class="title">HiSMatch: Historical Structure Matching based Temporal Knowledge Graph Reasoning</div> <div class="author"> <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Zhongni Hou,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  Weihua Peng,  <span class="font-weight-bold">Long Bai</span>,  Yajuan Lyu ,  Wei Li,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: EMNLP 2022</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.findings-emnlp.542" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/Lee-zix/HiSMatch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>A Temporal Knowledge Graph (TKG) is a sequence of KGs with respective timestamps, which adopts quadruples in the form of (subject, relation, object, timestamp) to describe dynamic facts. TKG reasoning has facilitated many real-world applications via answering such queries as (query entity, query relation, ?, future timestamp) about future. This is actually a matching task between a query and candidate entities based on their historical structures, which reflect behavioral trends of the entities at different timestamps. In addition, recent KGs provide background knowledge of all the entities, which is also helpful for the matching. Thus, in this paper, we propose the Historical Structure Matching (HiSMatch) model. It applies two structure encoders to capture the semantic information contained in the historical structures of the query and candidate entities. Besides, it adopts another encoder to integrate the background knowledge into the model. TKG reasoning experiments on six benchmark datasets demonstrate the significant improvement of the proposed HiSMatch model, with up to 5.6% performance improvement in MRR, compared to the state-of-the-art baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://coling2022.org/" rel="external nofollow noopener" target="_blank">COLING-2022</a> </abbr> </div> <div id="coling-2022" class="col-sm-8"> <div class="title">Meta-CQG: A Meta-Learning Framework for Complex Question Generation over Knowledge Bases</div> <div class="author"> Kun Zhang,  Yunqi Qiu,  Yuanzhuo Wang,  <span class="font-weight-bold">Long Bai</span> ,  Wei Li,  Xuhui Jiang,  Huawei Shen,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 29th International Conference on Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.coling-1.533" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Complex question generation over knowledge bases (KB) aims to generate natural language questions involving multiple KB relations or functional constraints. Existing methods train one encoder-decoder-based model to fit all questions. However, such a one-size-fits-all strategy may not perform well since complex questions exhibit an uneven distribution in many dimensions, such as question types, involved KB relations, and query structures, resulting in insufficient learning for long-tailed samples under different dimensions. To address this problem, we propose a meta-learning framework for complex question generation. The meta-trained generator can acquire universal and transferable meta-knowledge and quickly adapt to long-tailed samples through a few most related training samples. To retrieve similar samples for each input query, we design a self-supervised graph retriever to learn distributed representations for samples, and contrastive learning is leveraged to improve the learned representations. We conduct experiments on both WebQuestionsSP and ComplexWebQuestion, and results on long-tailed samples of different dimensions have been significantly improved, which demonstrates the effectiveness of the proposed framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AES-2022</abbr> </div> <div id="aes-2022" class="col-sm-8"> <div class="title">面向知识库问答的问句语义解析研究综述</div> <div class="author"> 仇韫琦,  王元卓,  <span class="font-weight-bold"> 白龙</span>,  尹芷仪,  沈华伟,  白硕 </div> <div class="periodical"> <em>电子学报</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ejournal.org.cn/CN/10.12263/DZXB.20220212" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>知识库问答（Knowledge Base Question Answering，KBQA）借助知识库中精度高、关联性强的结构化知识，为给定的复杂事实型问句提供准确、简短的答案.语义解析是知识库问答的主流方法之一，该类方法在给定的问句语义表征形式下，将非结构化的问句映射为结构化的语义表征，再将其改写为知识库查询获取答案。目前，面向知识库问答的语义解析方法主要面临三个挑战：首先是如何选择合适的语义表征形式以表达问句的语义，然后是如何解析问句的复杂语义并输出相应的语义表征，最后是如何应对特定领域中数据标注成本高昂、高质量数据匮乏的问题.本文从上述挑战出发，分析了知识库问答中常用的语义表征的特点与不足，然后梳理现有方法并总结分析其如何应对问句的复杂语义，接着介绍了当前方法在标注数据匮乏的低资源场景下的尝试，最后展望并讨论了面向知识库问答的语义解析的未来发展方向。</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TKDE-2022</abbr> </div> <div id="tkde-2022" class="col-sm-8"> <div class="title">What is Event Knowledge Graph: A Survey</div> <div class="author"> Saiping Guan,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a>,  <span class="font-weight-bold">Long Bai</span>,  Fujun Zhang,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Yutao Zeng,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a> </div> <div class="periodical"> <em>IEEE Transactions on Knowledge and Data Engineering</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9792280" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Besides entity-centric knowledge, usually organized as Knowledge Graph (KG), events are also an essential kind of knowledge in the world, which trigger the spring up of event-centric knowledge representation form like Event KG (EKG). It plays an increasingly important role in many downstream applications, such as search, question-answering, recommendation, financial quantitative investments, and text generation. This paper provides a comprehensive survey of EKG from history, ontology, instance, and application views. Specifically, to characterize EKG thoroughly, we focus on its history, definition, schema induction, acquisition, related representative graphs/systems, and applications. The development processes and trends are studied therein. We further summarize prospective directions to facilitate future research on EKG.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.2022.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL-2022</a> </abbr> </div> <div id="acl-2022" class="col-sm-8"> <div class="title">Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning</div> <div class="author"> <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  Weihua Peng,  Yajuan Lyu,  Yong Zhu,  <span class="font-weight-bold">Long Bai</span> ,  Wei Li,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.acl-short.32" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and time-variability. Existing models for TKG reasoning focus on modeling fact sequences of a fixed length, which cannot discover complex evolutional patterns that vary in length. Furthermore, these models are all trained offline, which cannot well adapt to the changes of evolutional patterns from then on. Thus, we propose a new model, called Complex Evolutional Network (CEN), which uses a length-aware Convolutional Neural Network (CNN) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy. Besides, we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time. Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2021.emnlp.org/" rel="external nofollow noopener" target="_blank">EMNLP-2021</a> </abbr> </div> <div id="emnlp-2021" class="col-sm-8"> <div class="title">Integrating Deep Event-Level and Script-Level Information for Script Event Prediction</div> <div class="author"> <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.emnlp-main.777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/waltbai/MCPredictor" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At the script level, most existing studies only consider a single event sequence corresponding to one common protagonist. In this paper, we propose a Transformer-based model, called MCPredictor, which integrates deep event-level and script-level information for script event prediction. At the event level, MCPredictor utilizes the rich information in the text to obtain more comprehensive event semantic representations. At the script-level, it considers multiple event sequences corresponding to different participants of the subsequent event. The experimental results on the widely-used New York Times corpus demonstrate the effectiveness and superiority of the proposed model.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://2021.aclweb.org/" rel="external nofollow noopener" target="_blank">ACL-2021</a> </abbr> </div> <div id="acl-2021" class="col-sm-8"> <div class="title">Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning</div> <div class="author"> Zhongni Hou,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <a href="https://lee-zix.github.io/" rel="external nofollow noopener" target="_blank">Zixuan Li</a>,  <span class="font-weight-bold">Long Bai</span> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.findings-acl.412" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Multi-hop reasoning is an effective and explainable approach to predicting missing facts in Knowledge Graphs (KGs). It usually adopts the Reinforcement Learning (RL) framework and searches over the KG to find an evidential path. However, due to the large exploration space, the RL-based model struggles with the serious sparse reward problem and needs to make a lot of trials. Moreover, its exploration can be biased towards spurious paths that coincidentally lead to correct answers. To solve both problems, we propose a simple but effective RL-based method called RARL (Rule-Aware RL). It injects high quality symbolic rules into the model’s reasoning process and employs partially random beam search, which can not only increase the probability of paths getting rewards, but also alleviate the impact of spurious paths. Experimental results show that it outperforms existing multi-hop methods in terms of Hit@1 and MRR.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.acml-conf.org/2020/" rel="external nofollow noopener" target="_blank">ACML-2020</a> </abbr> </div> <div id="acml-2020" class="col-sm-8"> <div class="title">Bidirectional Dependency-Guided Attention for Relation Extraction</div> <div class="author"> Xingchen Deng,  Lei Zhang,  Yixing Fan,  <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/rcpy/bssds/202203/t20220308_20651.html" rel="external nofollow noopener" target="_blank">Jiafeng Guo</a>,  Pengfei Wang </div> <div class="periodical"> <em>In Proceedings of the 12th Asian Conference on Machine Learning</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v129/deng20a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The dependency relation between words in the sentence is critical for the relation extraction. Existing methods often utilize the dependencies accompanied with various pruning strategies, thus suffer from the loss of detailed semantic information.In order to exploit dependency structure more effectively, we propose a novel bidirectional dependency-guided attention model. The main idea is to use a top-down attention as well as a bottom-up attention to fully capture the dependencies from different granularity. Specifically, the bottom-up attention aims to model the local semantics from the subtree of each node, while the top-down attention is to model the global semantics from the ancestor nodes. Moreover, we employ a label embedding component to attend the contextual features, which are extracted by the dependency-guided attention. Overall, the proposed model is fully attention-based which make it easy for parallel computing. Experiment results on TACRED dataset and SemEval 2010 Task 8 dataset show that our model outperforms existing dependency based models as well as the powerful pretraining model. Moreover, the proposed model achieves the state-of-the-art performance on TACRED dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://www.cikm2020.org" rel="external nofollow noopener" target="_blank">CIKM-2020</a> </abbr> </div> <div id="cikm-2020" class="col-sm-8"> <div class="title">Hierarchical Query Graph Generation for Complex Question Answering over Knowledge Graph</div> <div class="author"> Yunqi Qiu,  Kun Zhang,  Yuanzhuo Wang,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  <span class="font-weight-bold">Long Bai</span>,  Saiping Guan,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/abs/10.1145/3340531.3411888" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Knowledge Graph Question Answering aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge graphs. When faced with a complex question with compositional semantics, query graph generation is a practical semantic parsing-based method. But existing works rely on heuristic rules with limited coverage, making them impractical on more complex questions. This paper proposes a Director-Actor-Critic framework to overcome these challenges. Through options over a Markov Decision Process, query graph generation is formulated as a hierarchical decision problem. The Director determines which types of triples the query graph needs, the Actor generates corresponding triples by choosing nodes and edges, and the Critic calculates the semantic similarity between the generated triples and the given questions. Moreover, to train from weak supervision, we base the framework on hierarchical Reinforcement Learning with intrinsic motivation. To accelerate the training process, we pre-train the Critic with high-reward trajectories generated by hand-crafted rules, and leverage curriculum learning to gradually increase the complexity of questions during query graph generation. Extensive experiments conducted over widely-used benchmark datasets demonstrate the effectiveness of the proposed framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100"> <a href="https://aaai.org/conference/aaai/aaai-20/" rel="external nofollow noopener" target="_blank">AAAI-2020</a> </abbr> </div> <div id="aaai-2020" class="col-sm-8"> <div class="title">Entity Type Enhanced Neural Model for Distantly Supervised Relation Extraction (Student Abstract)</div> <div class="author"> <span class="font-weight-bold">Long Bai</span>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank">Xiaolong Jin</a>,  Chuanzhi Zhuang,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank">Xueqi Cheng</a> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/7147" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Distantly Supervised Relation Extraction (DSRE) has been widely studied, since it can automatically extract relations from very large corpora. However, existing DSRE methods only use little semantic information about entities, such as the information of entity type. Thus, in this paper, we propose a method for integrating entity type information into a neural network based DSRE model. It also adopts two attention mechanisms, namely, sentence attention and type attention. The former selects the representative sentences for a sentence bag, while the latter selects appropriate type information for entities. Experimental comparison with existing methods on a benchmark dataset demonstrates its merits.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JCIP-2019</abbr> </div> <div id="jcip-2019-1" class="col-sm-8"> <div class="title">基于深度学习的关系抽取研究综述</div> <div class="author"> 庄传志,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank"> 靳小龙</a>,  朱伟建,  刘静伟,  <span class="font-weight-bold"> 白龙</span>,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank"> 程学旗</a> </div> <div class="periodical"> <em>中文信息学报</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://jcip.cipsc.org.cn/CN/Y2019/V33/I12/1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>关系抽取（RE）是为了抽取文本中包含的关系，是信息抽取（IE）的重要组成部分。近年来，研究人员利用深度学习技术在该领域开展了深入研究。由于神经网络类型丰富，基于深度学习的关系抽取方法也更加多样。该文从关系抽取的基本概念出发，对关系抽取方法依据不同的视角进行了类别划分。随后，介绍了基于深度学习的关系抽取方法常用的数据集，并总结出基于深度学习的关系抽取框架。在此框架下，对关系抽取方法在面向深度学习的输入数据预处理、面向深度学习的神经网络模型设计等方面的具体工作进行了分析与评述，最后对未来的研究方向进行了探讨和展望。</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JCIP-2019</abbr> </div> <div id="jcip-2019-2" class="col-sm-8"> <div class="title">基于远程监督的关系抽取研究综述</div> <div class="author"> <span class="font-weight-bold"> 白龙</span>,  <a href="https://bigdatalab.ac.cn/yjdw/yjyzgg/202203/t20220308_20670.html" rel="external nofollow noopener" target="_blank"> 靳小龙</a>,  席鹏弼,  <a href="https://bigdatalab.ac.cn/yjdw/jcrc/202203/t20220308_20663.html" rel="external nofollow noopener" target="_blank"> 程学旗</a> </div> <div class="periodical"> <em>中文信息学报</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://jcip.cipsc.org.cn/CN/Y2019/V33/I10/10" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>关系抽取作为信息抽取的一项关键技术，在知识库自动构建、问答系统等领域有着极为重要的意义，一直以来受到人们的关注。远程监督关系抽取技术通过外部知识库作为监督源，自动对语料库进行标注，能够大量节省人工标注成本，因而受到了研究者们的重视。该文针对远程监督关系抽取技术做了较为系统性的梳理，将已有方法分为基于概率图的、基于矩阵补全的和基于嵌入的三大类，并且对其当前面临的挑战进行了探讨，最后总结并展望了远程监督关系抽取技术未来的发展。</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/george-gca/multi-language-al-folio" rel="external nofollow noopener" target="_blank">multi-language-al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>