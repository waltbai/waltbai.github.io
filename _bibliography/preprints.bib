---
---

@string{aps = {American Physical Society,}}

@misc{arxiv-2023-2,
  abbr         = {arXiv},
  html         = {https://arxiv.org/abs/2311.02962},
  title        = {Retrieval-Augmented Code Generation for Universal Information Extraction},
  author       = {Guo, Yucan and 
                  Li, Zixuan and 
                  Jin, Xiaolong and 
                  Liu, Yantao and 
                  Zeng, Yutao and
                  Liu, Wenxuan and 
                  Li, Xiang and 
                  Yang, Pan and 
                  Bai, Long and 
                  Guo, Jiafeng and 
                  Cheng, Xueqi },
  abstract     = {Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.},
  howpublished = {arXiv preprint},
  month        = nov,
  year         = {2023}
}

@misc{arxiv-2023-1,
  abbr         = {arXiv},
  html         = {https://arxiv.org/abs/2309.12892},
  title        = {ProtoEM: A Prototype-Enhanced Matching Framework for Event Relation Extraction},
  author       = {Hu, Zhilei and
                  Li, Zixuan and 
                  Xu, Daozhu and 
                  Bai, Long and 
                  Jin, Cheng and 
                  Jin, Xiaolong and 
                  Guo, Jiafeng and 
                  Cheng, Xueqi  },
  abstract     = {Event Relation Extraction (ERE) aims to extract multiple kinds of relations among events in texts. However, existing methods singly categorize event relations as different classes, which are inadequately capturing the intrinsic semantics of these relations. To comprehensively understand their intrinsic semantics, in this paper, we obtain prototype representations for each type of event relation and propose a Prototype-Enhanced Matching (ProtoEM) framework for the joint extraction of multiple kinds of event relations. Specifically, ProtoEM extracts event relations in a two-step manner, i.e., prototype representing and prototype matching. In the first step, to capture the connotations of different event relations, ProtoEM utilizes examples to represent the prototypes corresponding to these relations. Subsequently, to capture the interdependence among event relations, it constructs a dependency graph for the prototypes corresponding to these relations and utilized a Graph Neural Network (GNN)-based module for modeling. In the second step, it obtains the representations of new event pairs and calculates their similarity with those prototypes obtained in the first step to evaluate which types of event relations they belong to. Experimental results on the MAVEN-ERE dataset demonstrate that the proposed ProtoEM framework can effectively represent the prototypes of event relations and further obtain a significant improvement over baseline models.},
  howpublished = {arXiv preprint},
  month        = sep,
  year         = {2023}
}
